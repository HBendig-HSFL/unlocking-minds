{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition and Evaluation\n",
    "## Table of Contents\n",
    "1. [Model Selection](#model-selection)\n",
    "2. [Feature Engineering](#feature-engineering)\n",
    "3. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Evaluation Metrics](#evaluation-metrics)\n",
    "6. [Comparative Analysis](#comparative-analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T18:24:28.161086Z",
     "start_time": "2024-06-12T18:24:28.157181Z"
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization\n",
    "from keras.src.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T18:27:04.447144Z",
     "start_time": "2024-06-12T18:27:04.391966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Beispiel Daten laden\n",
    "data = pd.read_csv('./../Data/preprocessed_df.csv') # Pfad zu Ihren Daten\n",
    "\n",
    "# Daten skalieren\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Dimension der Eingabedaten\n",
    "input_dim = data_scaled.shape[1]\n",
    "\n",
    "print(data_scaled[:5])\n",
    "\n",
    "# Laden der Daten\n",
    "data = data_scaled \n",
    "latent_dim = 20  # Dimensionalität des latenten Raums\n",
    "\n",
    "\n",
    "# Definieren der Encoder-Architektur\n",
    "encoder_inputs = keras.Input(shape=(data.shape[-1],))\n",
    "x = layers.Dense(128, activation=\"relu\")(encoder_inputs)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim)(x)\n",
    "z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "# Definieren der Decoder-Architektur\n",
    "decoder_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(64, activation=\"relu\")(decoder_inputs)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "decoder_outputs = layers.Dense(data.shape[-1])(x)\n",
    "\n",
    "# Erstellen des VAE-Modells\n",
    "vae = keras.Model(encoder_inputs, decoder_outputs)\n",
    "\n",
    "# Trainieren des VAE-Modells\n",
    "vae.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.MeanSquaredError())\n",
    "vae.fit(data, data, epochs=50, batch_size=1)\n",
    "\n",
    "# Generieren neuer Daten\n",
    "new_data = vae.decode(tf.random.normal((1000, latent_dim)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Exception encountered when calling Functional.call().\\n\\n\\x1b[1m13171555600\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=tf.Tensor(shape=(1, 14), dtype=float32)\\n  • training=True\\n  • mask=None'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Trainieren des VAE-Modells\u001B[39;00m\n\u001B[1;32m      2\u001B[0m vae\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39mkeras\u001B[38;5;241m.\u001B[39moptimizers\u001B[38;5;241m.\u001B[39mAdam(), loss\u001B[38;5;241m=\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlosses\u001B[38;5;241m.\u001B[39mMeanSquaredError())\n\u001B[0;32m----> 3\u001B[0m \u001B[43mvae\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Generieren neuer Daten\u001B[39;00m\n\u001B[1;32m      6\u001B[0m new_data \u001B[38;5;241m=\u001B[39m vae\u001B[38;5;241m.\u001B[39mdecode(tf\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mnormal((\u001B[38;5;241m1000\u001B[39m, latent_dim)))\n",
      "File \u001B[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/src/ops/function.py:163\u001B[0m, in \u001B[0;36mFunction._run_through_graph\u001B[0;34m(self, inputs, operation_fn, call_fn)\u001B[0m\n\u001B[1;32m    161\u001B[0m output_tensors \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutputs:\n\u001B[0;32m--> 163\u001B[0m     output_tensors\u001B[38;5;241m.\u001B[39mappend(\u001B[43mtensor_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mid\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tree\u001B[38;5;241m.\u001B[39mpack_sequence_as(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_outputs_struct, output_tensors)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'Exception encountered when calling Functional.call().\\n\\n\\x1b[1m13171555600\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=tf.Tensor(shape=(1, 14), dtype=float32)\\n  • training=True\\n  • mask=None'"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
